{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iHah9Vq74t0e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-bs1_g342Nh",
    "outputId": "8e6e23d9-bf01-4530-dce5-1442ec442ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-18 17:55:33--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.167.184, 52.216.209.24, 52.217.160.168, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.167.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 600901 (587K) [text/plain]\n",
      "Saving to: ‘nietzsche.txt.2’\n",
      "\n",
      "nietzsche.txt.2     100%[===================>] 586.82K  1.02MB/s    in 0.6s    \n",
      "\n",
      "2023-06-18 17:55:34 (1.02 MB/s) - ‘nietzsche.txt.2’ saved [600901/600901]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArE9Sysh5EDE",
    "outputId": "ae2671ba-c31b-4b86-e797-e012e256e790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 600893\n"
     ]
    }
   ],
   "source": [
    "with open('nietzsche.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('length:', len(text))\n",
    "text = re.sub('[^a-z ]', ' ', text)\n",
    "text = re.sub('\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Ijyo7gcL49kz",
    "outputId": "738b82e2-1ea3-41d0-ba9f-422ae4b659af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iNiH2xET5HxF"
   },
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR = sorted(list(set(text)))\n",
    "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAer4W2RYfhr",
    "outputId": "964f954a-7b56-4ed4-a178-5d3fd87623cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4EeJBub5ueL",
    "outputId": "6e6de311-5399-43e8-e49b-4e708eebeb46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sents: 193075\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "STEP = 3\n",
    "SENTENCES = []\n",
    "NEXT_CHARS = []\n",
    "for i in range(0, len(text) - MAX_LEN, STEP):\n",
    "    SENTENCES.append(text[i: i + MAX_LEN])\n",
    "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
    "print('Num sents:', len(SENTENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHPHQII_6MUV",
    "outputId": "d48cb0da-193f-478e-e011-0cc3fa05c82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
    "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
    "for i, sentence in enumerate(SENTENCES):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = CHAR_TO_INDEX[char]\n",
    "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7MP7Jzi7PAN",
    "outputId": "c6d57fcb-d912-4b43-a2e8-17a46e4a8003"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
       "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
       "          13,  1, 14,  0]]),\n",
       " tensor(23))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4XKb2CyB6nwL"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=512\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "psIcSGM27YPL"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)\n",
    "        predictions = self.output(state[0].squeeze())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wHDuSE8A7ssc"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTCG-ESC74UK",
    "outputId": "183a612f-2a3a-4a5a-87a7-f355a47f924f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0124,  0.1266, -0.1151, -0.0075, -0.0161,  0.0299, -0.0930, -0.0228,\n",
       "         0.1060,  0.0493, -0.0202, -0.0397,  0.0827, -0.0721,  0.0468,  0.0895,\n",
       "         0.0609,  0.0054, -0.1286,  0.0954,  0.0660,  0.2153, -0.0481, -0.0264,\n",
       "        -0.0420,  0.0932, -0.0165], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gch6FQl8x6Hj"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(INDEX_TO_CHAR), 15)\n",
    "rnn = nn.LSTM(15,128, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJsUqfxYeq0b",
    "outputId": "fef78aac-7f25-4225-f2e0-37a661d91676"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 40, 128]),\n",
       " 2,\n",
       " torch.Size([1, 10, 128]),\n",
       " torch.Size([1, 10, 128]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, s = rnn(embedding(X[0:10]))\n",
    "o.shape, len(s), s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qbiqECBCP4Bv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 40, 128]), 1, torch.Size([10, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.GRU(15,128, batch_first=True)\n",
    "o, s = rnn(embedding(X[0:10]))\n",
    "o.shape, len(s), s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "evDHlyNOykBr"
   },
   "outputs": [],
   "source": [
    "o, s = rnn(embedding(X[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZQpkKJV_76dq"
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    softmaxed = torch.softmax(preds, 0)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
    "    return probas.argmax()\n",
    "\n",
    "def generate_text():\n",
    "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + MAX_LEN]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
    "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
    "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
    "\n",
    "        preds = model(x_pred)\n",
    "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
    "        generated = generated + next_char\n",
    "\n",
    "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([51,50,1,49,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for i in a :\n",
    "  p.append(torch.exp(i)/torch.sum(torch.exp(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.FloatTensor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29., 16.,  0.,  5.,  0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.multinomial.Multinomial (50,p).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hylQYY8H_Lw2"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qshorynU9-Cx",
    "outputId": "e2b45154-9cf2-4799-d66a-2eb9fc5a92fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 17.880, Train loss: 2.175\n",
      "o be old fashioned and grandfatherly res|enas a ans at of he worsttlive and onsel\n",
      "Epoch 1. Time: 18.955, Train loss: 1.795\n",
      "ble filth of all political agitation the| vires theveres of persines a onlest tha\n",
      "Epoch 2. Time: 17.772, Train loss: 1.656\n",
      "ganizations how the dim mole eyes of suc|klic our it in grole in to the orlies wi\n",
      "Epoch 3. Time: 18.977, Train loss: 1.569\n",
      "nward impulse rules them with the master| a narder arted they in which knownsh to\n",
      "Epoch 4. Time: 18.136, Train loss: 1.506\n",
      "uld be philosophers nowadays would be co|nernow oun of ording to the drome to may\n",
      "Epoch 5. Time: 18.743, Train loss: 1.460\n",
      "osite but as its refinement it is to be |philosomptent with reart and the apsaine\n",
      "Epoch 6. Time: 19.571, Train loss: 1.423\n",
      "ere are systems of morals which are mean|s the en arthain pectleness to the stold\n",
      "Epoch 7. Time: 18.369, Train loss: 1.391\n",
      "et a german who was favourably inclined |close and the world bad one consciousent\n",
      "Epoch 8. Time: 19.851, Train loss: 1.367\n",
      "ing to men and even to philosophers perh|acidemently corracieracred his but it th\n",
      "Epoch 9. Time: 17.801, Train loss: 1.345\n",
      "re that which we now call life and exper|ience so the sightens which is of hour v\n",
      "Epoch 10. Time: 19.838, Train loss: 1.326\n",
      "hings must prevail of itself in accordan|ding ruspenes it all was themses find on\n",
      "Epoch 11. Time: 18.332, Train loss: 1.309\n",
      "ugh the streets into dung heaps and gutt|ly quepsiled and speeming in the wanthot\n",
      "Epoch 12. Time: 18.924, Train loss: 1.294\n",
      "acted mythologically the non free will i|n long to delighs soling of world existe\n",
      "Epoch 13. Time: 18.750, Train loss: 1.280\n",
      " and which we are sensible of even in so|lto it is a seiginal namely the stuptin \n",
      "Epoch 14. Time: 19.503, Train loss: 1.267\n",
      "the world of men in so far as it has mad|e lost because to pirte equally as again\n",
      "Epoch 15. Time: 18.176, Train loss: 1.257\n",
      "herto lingered obscure well worth questi|mate assement in the misuntinis of nhopi\n",
      "Epoch 16. Time: 18.815, Train loss: 1.245\n",
      "wer effects its ultimate consequences ev|en their friends and scysonatiy the cent\n",
      "Epoch 17. Time: 19.908, Train loss: 1.235\n",
      " sensed in it belongs to its surface or |love many has to consequently more plait\n",
      "Epoch 18. Time: 18.769, Train loss: 1.226\n",
      "d on paths of thought different from tho|ught these impulse which because is fait\n",
      "Epoch 19. Time: 20.531, Train loss: 1.217\n",
      "nd sublimity of moral questionableness a|s would of the apty long laws the occasi\n",
      "Epoch 20. Time: 18.991, Train loss: 1.208\n",
      "ess is generally looked upon as affordin|g and act sportent and breand an demoner\n",
      "Epoch 21. Time: 19.444, Train loss: 1.201\n",
      "and rigorous nobleness and self responsi|ble fur feel an early men one use and sc\n",
      "Epoch 22. Time: 18.949, Train loss: 1.193\n",
      "owing to a new self consciousness and ac|tionjsities the saint makes man always w\n",
      "Epoch 23. Time: 19.731, Train loss: 1.185\n",
      "on referred to may indeed be great it is| this explianing of suffer and so must a\n",
      "Epoch 24. Time: 17.907, Train loss: 1.178\n",
      "verything absolute belongs to pathology |and part to do whotest devility perpulen\n",
      "Epoch 25. Time: 20.183, Train loss: 1.171\n",
      "n he has ever been in germany not to spe|aking in dangerously at if the person sh\n",
      "Epoch 26. Time: 18.178, Train loss: 1.166\n",
      "ng his own arts and artifices for self p|ower they alcer awreather only his revol\n",
      "Epoch 27. Time: 19.225, Train loss: 1.160\n",
      "ster stroke of religions and metaphysics| of the very sensimentious and historica\n",
      "Epoch 28. Time: 19.129, Train loss: 1.152\n",
      "exercised its acuteness and profundity h|appiness all think for honess stupidal r\n",
      "Epoch 29. Time: 18.419, Train loss: 1.148\n",
      "fixes itself exclusively on one thing th|at is chusival or souns in the portalism\n",
      "Epoch 30. Time: 19.330, Train loss: 1.142\n",
      "ng of the too rigid symmetry as intentio|nes of the appears of fashion aswardes t\n",
      "Epoch 31. Time: 17.840, Train loss: 1.137\n",
      "urs the fatherland the earth the dignity| to favour equal that our who always com\n",
      "Epoch 32. Time: 19.350, Train loss: 1.131\n",
      "eceive ourselves about it wherever the i|ndividual of wantion is a before chearfu\n",
      "Epoch 33. Time: 18.323, Train loss: 1.125\n",
      "will has been long stored up and accumul|ary period disquestfully wanto there is \n",
      "Epoch 34. Time: 19.907, Train loss: 1.121\n",
      " in every sense may perhaps one day be t|y in morally regard to be long forced op\n",
      "Epoch 35. Time: 18.724, Train loss: 1.117\n",
      "s and sudden outbreaks and that in its u|pon it is usual they are real as firt it\n",
      "Epoch 36. Time: 19.463, Train loss: 1.113\n",
      "oy and sorrow from the beginning a sort |notions life in the immelliness for inst\n",
      "Epoch 37. Time: 18.818, Train loss: 1.107\n",
      "hat even in its domain the most magnific|t the greatest cosmanic science and mora\n",
      "Epoch 38. Time: 18.978, Train loss: 1.102\n",
      "ciates who so domineered him with look a|nd rather so philologis freedom but to i\n",
      "Epoch 39. Time: 20.232, Train loss: 1.097\n",
      "he selecting and disciplining influence |the very near to havi of capable love to\n",
      "Epoch 40. Time: 18.840, Train loss: 1.093\n",
      " spirit which awakened europe out of its| explative man cause the contemplation a\n",
      "Epoch 41. Time: 19.237, Train loss: 1.089\n",
      " self preservation but in requital this |peoples that goes at as it is disepty s \n",
      "Epoch 42. Time: 20.145, Train loss: 1.085\n",
      "e must take the consequences of his faul|ties whom because we their something gen\n",
      "Epoch 43. Time: 19.077, Train loss: 1.080\n",
      "er only such men as could express simila|r him to punsof the judgmentitic a credi\n",
      "Epoch 44. Time: 21.275, Train loss: 1.077\n",
      " byron has put this into deathless verse| and ventures of man to a every ank reso\n",
      "Epoch 45. Time: 18.811, Train loss: 1.073\n",
      "lso of the loathsome and dreadful still |intentious circumstances itself by means\n",
      "Epoch 46. Time: 20.207, Train loss: 1.069\n",
      "authoritative inasmuch as mankind or the| free slavery eternal let the deepest se\n",
      "Epoch 47. Time: 20.407, Train loss: 1.066\n",
      " cannot put himself in the place of the |entire the greatness revered contemplate\n",
      "Epoch 48. Time: 18.324, Train loss: 1.060\n",
      "se they did not know the higher nature b|ettern we latest foothous european scion\n",
      "Epoch 49. Time: 19.287, Train loss: 1.058\n",
      " man of learning the scientific average |able they were do so together as the fir\n",
      "Epoch 50. Time: 19.259, Train loss: 1.055\n",
      "iend no do not ask me who at midday twas| may naturalism and depths oneself and g\n",
      "Epoch 51. Time: 18.932, Train loss: 1.051\n",
      "ed diverse sensitive and refined as the |joy of their virtually charferfulness va\n",
      "Epoch 52. Time: 20.117, Train loss: 1.046\n",
      " creative plenipotence and lordliness at| cases the too could say in the partin t\n",
      "Epoch 53. Time: 18.544, Train loss: 1.043\n",
      " the greatest feats of the men who are c|ircates and philosophy as a system to re\n",
      "Epoch 54. Time: 19.124, Train loss: 1.041\n",
      "or thee my good neighbour instinct when |their better the picutonity the conscien\n",
      "Epoch 55. Time: 18.746, Train loss: 1.037\n",
      "rce was flickering out in brilliant fire| lobgent of the world with the question \n",
      "Epoch 56. Time: 18.824, Train loss: 1.034\n",
      "g hunt and also the great danger commenc|e of weaker a mirmlarce he who is in the\n",
      "Epoch 57. Time: 19.053, Train loss: 1.030\n",
      "or to speak more plainly physiological d|efecticity of the old interpretation as \n",
      "Epoch 58. Time: 18.392, Train loss: 1.026\n",
      "he localised nature of the rule of every| no in germany there is not to socration\n",
      "Epoch 59. Time: 19.016, Train loss: 1.023\n",
      "e drawn together only such men as could |not being this bard within succivations \n",
      "Epoch 60. Time: 18.785, Train loss: 1.019\n",
      "d sombre garb and silence meet dress for| highest mistualized than the anti knowl\n",
      "Epoch 61. Time: 20.106, Train loss: 1.017\n",
      "ch religions and to bring to light their| ye dispoting of which came himself in s\n",
      "Epoch 62. Time: 19.245, Train loss: 1.014\n",
      "the newspaper reading demimonde of intel|lectual praise and lust at once delight \n",
      "Epoch 63. Time: 20.067, Train loss: 1.011\n",
      "al hopes through the peace of soul there|fore my furtall which the most and some \n",
      "Epoch 64. Time: 18.917, Train loss: 1.009\n",
      "he uncomprehended fearful mysterious nat|ain the entire appeasing and better powe\n",
      "Epoch 65. Time: 19.838, Train loss: 1.005\n",
      "rtiality for seeing the cause of almost |only as the ascention see for herved a w\n",
      "Epoch 66. Time: 18.564, Train loss: 1.002\n",
      "stomed to disregard this duality and to |assage of word and must be rest repessio\n",
      "Epoch 67. Time: 19.688, Train loss: 0.999\n",
      "ain as the occasions of those lights and| its prosimble observe of god and glate \n",
      "Epoch 68. Time: 18.882, Train loss: 0.996\n",
      "s quite different for various ways and m|any great adoper to me to be under the h\n",
      "Epoch 69. Time: 19.338, Train loss: 0.994\n",
      " in dreams provided we experience it oft|en cordler already interebyratwise the w\n",
      "Epoch 70. Time: 18.416, Train loss: 0.990\n",
      "ch boldness in inversion nor anything at| present and concerned minds subject be \n",
      "Epoch 71. Time: 20.668, Train loss: 0.988\n",
      "i meant to say that let german depth be |who habil of beside himself no much i ma\n",
      "Epoch 72. Time: 19.520, Train loss: 0.985\n",
      "d of the poor man a delusion he thinks t|herefore themselves self burndarys of si\n",
      "Epoch 73. Time: 19.385, Train loss: 0.984\n",
      "sense the man of learning the scientific| play the germans him there are pradicit\n",
      "Epoch 74. Time: 18.530, Train loss: 0.979\n",
      " or is assigned is something that will b|ut calls which is determinia certainters\n",
      "Epoch 75. Time: 20.509, Train loss: 0.977\n",
      "revalent which we no longer share and wh|at wany of the christian rights he formm\n",
      "Epoch 76. Time: 18.799, Train loss: 0.975\n",
      "le profession and as i have said his who|le in prove good fear and revenges the o\n",
      "Epoch 77. Time: 19.373, Train loss: 0.974\n",
      "rns whatever he finds veiled or protecte|d the stake to him suffers feeling itsel\n",
      "Epoch 78. Time: 19.917, Train loss: 0.971\n",
      "and which is almost incredible is render|ed still refinements services the diffic\n",
      "Epoch 79. Time: 20.375, Train loss: 0.968\n",
      "of artists homer is so much at home amon|g himself those become a untone for a hu\n",
      "Epoch 80. Time: 18.521, Train loss: 0.966\n",
      "e eye every hand pressure every courtesy| with manifestution and saint and amsere\n",
      "Epoch 81. Time: 19.038, Train loss: 0.964\n",
      " a ruling idea which together with germa|n scriest just there first of our very w\n",
      "Epoch 82. Time: 18.860, Train loss: 0.961\n",
      "style in morality the fearfulness and ma|de as it is refinement of men where ther\n",
      "Epoch 83. Time: 19.292, Train loss: 0.959\n",
      "s quite down below in view of this liber|ations of victory if pity superior diffe\n",
      "Epoch 84. Time: 19.041, Train loss: 0.959\n",
      "depending on human will and to make prep|arents of question every philosophy befo\n",
      "Epoch 85. Time: 18.907, Train loss: 0.954\n",
      "needs a mask nay more around every profo|und generally a street non in his consid\n",
      "Epoch 86. Time: 20.615, Train loss: 0.952\n",
      "orality gains the ascendancy language sh|mid in his favouradly is except to honou\n",
      "Epoch 87. Time: 18.474, Train loss: 0.952\n",
      "o er the fatal truth the tree of knowled|ge is each explainess and the german eff\n",
      "Epoch 88. Time: 21.437, Train loss: 0.949\n",
      "hings and enjoins upon them other things| have agistion and degicable case in the\n",
      "Epoch 89. Time: 20.639, Train loss: 0.947\n",
      "ound stronger more evil and more profoun|dlysidity which rearing storepers bedorm\n",
      "Epoch 90. Time: 63.732, Train loss: 0.945\n",
      "by the most morbid and dangerous kind of| our good too those development bobled g\n",
      "Epoch 91. Time: 25.915, Train loss: 0.942\n",
      "another question why is belief in such j|ust the higher and benefochings for beli\n",
      "Epoch 92. Time: 20.466, Train loss: 0.940\n",
      "ot the world which concerns us be a fict|iman brought with its experience in whic\n",
      "Epoch 93. Time: 19.413, Train loss: 0.938\n",
      "ould fain give and bestow the noble man |concerns who assume of explanablent but \n",
      "Epoch 94. Time: 19.991, Train loss: 0.938\n",
      "standards of duration or measurement but| if ehze warnt time uppromblent of natur\n",
      "Epoch 95. Time: 18.407, Train loss: 0.936\n",
      "n in his dreams and that at last as soon| gratity thinking is not oft our great m\n",
      "Epoch 96. Time: 20.209, Train loss: 0.934\n",
      "our belief in this is just our belief in| druder and some thing that are nor ulav\n",
      "Epoch 97. Time: 18.384, Train loss: 0.934\n",
      "d indifferent magnificence which is shoc|her and must of the seems to the origin \n",
      "Epoch 98. Time: 20.729, Train loss: 0.932\n",
      "seem to be a rendezvous of questions and| own exception of this determines it is \n",
      "Epoch 99. Time: 19.294, Train loss: 0.929\n",
      "ong but not to myself as is sufficiently| geniused they are a this trandize to pr\n"
     ]
    }
   ],
   "source": [
    "for ep in range(100):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X_b, y_b in data:\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        answers = model(X_b)\n",
    "        loss = criterion(answers, y_b)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "    model.eval()\n",
    "    generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzUTHB9O_H6J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Занятие 6. Рекуррентные сети 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
