{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Занятие 6. Рекуррентные сети 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHah9Vq74t0e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-bs1_g342Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6e23d9-bf01-4530-dce5-1442ec442ad6"
      },
      "source": [
        "!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-09 17:18:34--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.59\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: ‘nietzsche.txt’\n",
            "\n",
            "\rnietzsche.txt         0%[                    ]       0  --.-KB/s               \rnietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-06-09 17:18:34 (15.8 MB/s) - ‘nietzsche.txt’ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArE9Sysh5EDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2671ba-c31b-4b86-e797-e012e256e790"
      },
      "source": [
        "with open('nietzsche.txt', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('length:', len(text))\n",
        "text = re.sub('[^a-z ]', ' ', text)\n",
        "text = re.sub('\\s+', ' ', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length: 600893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijyo7gcL49kz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "738b82e2-1ea3-41d0-ba9f-422ae4b659af"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNiH2xET5HxF"
      },
      "source": [
        "INDEX_TO_CHAR = sorted(list(set(text)))\n",
        "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAer4W2RYfhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964f954a-7b56-4ed4-a178-5d3fd87623cd"
      },
      "source": [
        "CHAR_TO_INDEX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " 'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4EeJBub5ueL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6de311-5399-43e8-e49b-4e708eebeb46"
      },
      "source": [
        "MAX_LEN = 40\n",
        "STEP = 3\n",
        "SENTENCES = []\n",
        "NEXT_CHARS = []\n",
        "for i in range(0, len(text) - MAX_LEN, STEP):\n",
        "    SENTENCES.append(text[i: i + MAX_LEN])\n",
        "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
        "print('Num sents:', len(SENTENCES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num sents: 193075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHPHQII_6MUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48cb0da-193f-478e-e011-0cc3fa05c82f"
      },
      "source": [
        "print('Vectorization...')\n",
        "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
        "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
        "for i, sentence in enumerate(SENTENCES):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t] = CHAR_TO_INDEX[char]\n",
        "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7MP7Jzi7PAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d57fcb-d912-4b43-a2e8-17a46e4a8003"
      },
      "source": [
        "X[0:1], Y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
              "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
              "          13,  1, 14,  0]]), tensor(23))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKb2CyB6nwL"
      },
      "source": [
        "BATCH_SIZE=512\n",
        "dataset = torch.utils.data.TensorDataset(X, Y)\n",
        "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psIcSGM27YPL"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
        "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
        "        self.output = nn.Linear(num_hiddens, num_classes)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        out = self.embedding(X)\n",
        "        _, state = self.hidden(out)\n",
        "        predictions = self.output(state[0].squeeze())\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHDuSE8A7ssc"
      },
      "source": [
        "model = NeuralNetwork(nn.GRU, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvKPD9L9zJal",
        "outputId": "8a69e7a2-ef45-4acb-af30-58ed5b20f316"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([193075, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTCG-ESC74UK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183a612f-2a3a-4a5a-87a7-f355a47f924f"
      },
      "source": [
        "model(X[0:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0107,  0.0403,  0.2283,  0.0933,  0.1282, -0.0314, -0.0694, -0.0393,\n",
              "        -0.1577,  0.0457, -0.0176, -0.1382, -0.1015,  0.0507,  0.0250, -0.1342,\n",
              "         0.2036,  0.0862,  0.1456,  0.1420,  0.2797,  0.0540, -0.1520,  0.0869,\n",
              "        -0.1288,  0.0291,  0.0249], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gch6FQl8x6Hj"
      },
      "source": [
        "embedding = nn.Embedding(len(INDEX_TO_CHAR), 15)\n",
        "rnn = nn.LSTM(15,128, batch_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, len(s), s[0].shape, s[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJsUqfxYeq0b",
        "outputId": "fef78aac-7f25-4225-f2e0-37a661d91676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]),\n",
              " 2,\n",
              " torch.Size([1, 10, 128]),\n",
              " torch.Size([1, 10, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.GRU(15,128, batch_first=True)\n",
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, len(s), s[0].shape"
      ],
      "metadata": {
        "id": "qbiqECBCP4Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evDHlyNOykBr"
      },
      "source": [
        "o, s = rnn(embedding(X[0:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hYTukwkykHJ",
        "outputId": "f75b46be-f6ab-4528-8e1c-b0e47dba0b06"
      },
      "source": [
        "o.shape, s[0].shape, s[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]), torch.Size([1, 10, 128]), torch.Size([1, 10, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbeKFkwdFclg"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQpkKJV_76dq"
      },
      "source": [
        "def sample(preds):\n",
        "    softmaxed = torch.softmax(preds, 0)\n",
        "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
        "    return probas.argmax()\n",
        "\n",
        "def generate_text():\n",
        "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
        "\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + MAX_LEN]\n",
        "    generated += sentence\n",
        "\n",
        "    for i in range(MAX_LEN):\n",
        "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
        "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
        "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
        "\n",
        "        preds = model(x_pred.cuda())[0].cpu()\n",
        "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
        "        generated = generated + next_char\n",
        "\n",
        "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EV09Ast97aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385ca42f-74ee-41a8-8aeb-3d12d30d195b"
      },
      "source": [
        "generate_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ts that they made interpreters for thems|jljxwhuo gprkpkuuknklypooxttqbtnhkzetlhc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hylQYY8H_Lw2"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qshorynU9-Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b45154-9cf2-4799-d66a-2eb9fc5a92fb"
      },
      "source": [
        "for ep in range(100):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_b, y_b in data:\n",
        "        X_b, y_b = X_b.cuda(), y_b.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_b)\n",
        "        loss = criterion(answers, y_b)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "    model.eval()\n",
        "    generate_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Time: 7.204, Train loss: 2.104\n",
            "gentle good hearted weak willed and poet|souch insaring inow oprays ham in hisgil\n",
            "Epoch 1. Time: 7.028, Train loss: 1.713\n",
            " and evil only that here and there perha|nder of wan their action alvess mad dirs\n",
            "Epoch 2. Time: 7.151, Train loss: 1.580\n",
            "ty as it is it resembles a well stacked |ant a hatiric light and doisterining if \n",
            "Epoch 3. Time: 7.156, Train loss: 1.502\n",
            " hands glances and delicate follies our |thinds of a do all one oneselves the bar\n",
            "Epoch 4. Time: 7.101, Train loss: 1.449\n",
            "stincts how jesuitical that amiable and |would comon of an old indivesking aying \n",
            "Epoch 5. Time: 7.035, Train loss: 1.409\n",
            "haps serve this age as its mirror and se|lf not did ruell the perhosequend not au\n",
            "Epoch 6. Time: 7.087, Train loss: 1.378\n",
            "kin as the bones flesh entrails and bloo|d skeld parting three soching they sense\n",
            "Epoch 7. Time: 7.121, Train loss: 1.353\n",
            "that the one who is guilty of them towar| to the betseas of a prcesuits with ordi\n",
            "Epoch 8. Time: 7.106, Train loss: 1.331\n",
            "r the good will to some refinement of in|finditives perhaps on the superituals to\n",
            "Epoch 9. Time: 7.093, Train loss: 1.312\n",
            " assumes the same right or to be more ac|t so little being ofly bad along and red\n",
            "Epoch 10. Time: 7.168, Train loss: 1.297\n",
            "istianity attains all its psychological |arrifine freedo they sagablime of their \n",
            "Epoch 11. Time: 7.060, Train loss: 1.283\n",
            "xpression current among painters in spit|ion which christian and condigioned supp\n",
            "Epoch 12. Time: 7.105, Train loss: 1.270\n",
            " assents to morality and plays the flute| as one fact and only socration of their\n",
            "Epoch 13. Time: 7.128, Train loss: 1.260\n",
            " to prostration before everything that w|ill to makes orgeded to wasser to artist\n",
            "Epoch 14. Time: 7.151, Train loss: 1.249\n",
            "annot do otherwise philosophy is this ty|us truble that make that them that as a \n",
            "Epoch 15. Time: 7.058, Train loss: 1.239\n",
            " motion which usually commence immediate|d sound there is know though symplation \n",
            "Epoch 16. Time: 7.085, Train loss: 1.230\n",
            " of human conduct his own included he ma|n our originated future and person that \n",
            "Epoch 17. Time: 7.086, Train loss: 1.222\n",
            "e most intellectual world the world appa|rently again author does not be true the\n",
            "Epoch 18. Time: 7.124, Train loss: 1.214\n",
            "erly those best heirs and scholars of as|seficed prioty the inashed too spirit id\n",
            "Epoch 19. Time: 7.069, Train loss: 1.206\n",
            " circumstances at first these spots were| delless of gands makes the creaclises t\n",
            "Epoch 20. Time: 7.114, Train loss: 1.200\n",
            " for him everything ponderous viscous an|d all the help by recommedabtour of holo\n",
            "Epoch 21. Time: 7.089, Train loss: 1.193\n",
            "y a nation in all souls a like number of| instance against sorthy at the metaphys\n",
            "Epoch 22. Time: 7.122, Train loss: 1.187\n",
            "e to shorten everything long even time i|n the undouscheformen that is different \n",
            "Epoch 23. Time: 7.102, Train loss: 1.181\n",
            "than myself the exception and he would g|low or quaire the signible suancentable \n",
            "Epoch 24. Time: 7.167, Train loss: 1.175\n",
            "hance by the forelock he who does not wi|th an aftimety of which is so avoubful a\n",
            "Epoch 25. Time: 7.223, Train loss: 1.170\n",
            " nothing in philosophy but a series of r|ather too had be europeans that the infl\n",
            "Epoch 26. Time: 7.032, Train loss: 1.165\n",
            "may be so great as to prevent our cleani|mically and looks in the thing sective i\n",
            "Epoch 27. Time: 7.091, Train loss: 1.160\n",
            "d to become distrustful also of all thin|g ambel man to means instinct of wholequ\n",
            "Epoch 28. Time: 7.065, Train loss: 1.155\n",
            "ly of impulses the less egoistic brought| and effacy are betweeved one myself whu\n",
            "Epoch 29. Time: 7.073, Train loss: 1.152\n",
            "ubstance in matter in the earth residuum|bes for him degreetful an emotion in the\n",
            "Epoch 30. Time: 7.083, Train loss: 1.148\n",
            "e outset for a similar development and s|omedoom events these there is not rifter\n",
            "Epoch 31. Time: 7.057, Train loss: 1.143\n",
            "openhauer believes himself able to prove| bully delicures one with its indignatur\n",
            "Epoch 32. Time: 7.087, Train loss: 1.139\n",
            " actions in the end however though silen|tary their stard by abyle and we have gi\n",
            "Epoch 33. Time: 6.991, Train loss: 1.135\n",
            "ereby to ariadne who was present in my o|n themsed anti caves life a questions th\n",
            "Epoch 34. Time: 7.081, Train loss: 1.133\n",
            " improbable that precisely this is natur|al fact the extent to say it is must the\n",
            "Epoch 35. Time: 7.099, Train loss: 1.129\n",
            "n out bad and sinful by nature to render| not circe for construe thing all of for\n",
            "Epoch 36. Time: 7.125, Train loss: 1.124\n",
            "hippodrome it is necessary to turn aroun|d metlocist and above always the demaced\n",
            "Epoch 37. Time: 7.094, Train loss: 1.121\n",
            "till wear their pigtail ah if you only k|nows partitie the most in its friends an\n",
            "Epoch 38. Time: 7.068, Train loss: 1.119\n",
            "heir charity parents involuntarily make |us or not to proceest enthoubtly be more\n",
            "Epoch 39. Time: 7.040, Train loss: 1.115\n",
            "hatever except in the usual exaggeration|s of called in the truth there are no ex\n",
            "Epoch 40. Time: 7.011, Train loss: 1.113\n",
            "cies of man the incurably mediocre the m|ost some really more as is a scepting th\n",
            "Epoch 41. Time: 7.031, Train loss: 1.109\n",
            " at all times eventually one must do eve|nts real all even where his love also we\n",
            "Epoch 42. Time: 7.095, Train loss: 1.106\n",
            "nd with capacity to develop the wise gui|stic easily each choribising educated as\n",
            "Epoch 43. Time: 7.118, Train loss: 1.103\n",
            "imself of life but keep on making himsel|f as let the valy of the said this disti\n",
            "Epoch 44. Time: 7.069, Train loss: 1.100\n",
            "t the question what is german never dies| bone whemous and different who take and\n",
            "Epoch 45. Time: 7.066, Train loss: 1.099\n",
            "lavery in some form or other without the| called than objection dull concealer of\n",
            "Epoch 46. Time: 7.076, Train loss: 1.096\n",
            "ghtful and daring nuances of free free s|pirits pletual qualitic of a heirs as so\n",
            "Epoch 47. Time: 7.014, Train loss: 1.093\n",
            " its own this operates fascinatingly per|haps there are the bad chromis they man \n",
            "Epoch 48. Time: 7.032, Train loss: 1.092\n",
            "ole nature man in the midst of nature is| ideal hell refined at to this fature fo\n",
            "Epoch 49. Time: 7.116, Train loss: 1.089\n",
            "rmly let us open our eyes and keep our h|ways our custicity something to justice \n",
            "Epoch 50. Time: 7.149, Train loss: 1.085\n",
            "than our egoism inasmuch as he must take| community jusng through a such a friend\n",
            "Epoch 51. Time: 7.070, Train loss: 1.083\n",
            "e us or was it we who presented ourselve|s s solent to long too coldering for eve\n",
            "Epoch 52. Time: 7.088, Train loss: 1.082\n",
            "w myself to rank philosophers according |to nowadly acquief these thing than crui\n",
            "Epoch 53. Time: 7.148, Train loss: 1.079\n",
            "ish with their profound mediocrity broug|ht the being years afterward supervation\n",
            "Epoch 54. Time: 7.008, Train loss: 1.077\n",
            "lising of justice and the beneficent sev|erites to fait there is grows a man phys\n",
            "Epoch 55. Time: 7.150, Train loss: 1.075\n",
            "customary in regard to the erotic injure|d who knows to flessens for truth quitcr\n",
            "Epoch 56. Time: 7.151, Train loss: 1.074\n",
            "en ought the lover of knowledge to heark|s philosopher to to be tyrances poorly w\n",
            "Epoch 57. Time: 7.151, Train loss: 1.072\n",
            " narrowing of perspectives and thus in a|re degenerating that they have as it lat\n",
            "Epoch 58. Time: 7.112, Train loss: 1.071\n",
            " of grace shed down upon him if he forme|r of the are all themselves instincts so\n",
            "Epoch 59. Time: 7.119, Train loss: 1.069\n",
            "claim to a superiority over europe is th|e conduct and time in a priotically more\n",
            "Epoch 60. Time: 7.101, Train loss: 1.067\n",
            " reason they have perhaps in times of da|ngerous recolvening that in the emotion \n",
            "Epoch 61. Time: 7.049, Train loss: 1.064\n",
            "st dangerous snares for human judgment a|nd experienced as i ultimate something o\n",
            "Epoch 62. Time: 7.099, Train loss: 1.064\n",
            "s of the middle classes especially in th|e one place and so many eligion which co\n",
            "Epoch 63. Time: 7.163, Train loss: 1.062\n",
            "ious tenets resembling in the way that c|lings of vain become secposing of resles\n",
            "Epoch 64. Time: 7.082, Train loss: 1.062\n",
            "eir own interest in oneself the wish to |guinds what think who has hitherto been \n",
            "Epoch 65. Time: 7.109, Train loss: 1.058\n",
            "t species of joy and thereby the source |overlow and tangs too through their if a\n",
            "Epoch 66. Time: 7.147, Train loss: 1.057\n",
            "mental problem of man and woman to deny |the classiblling with greatest freedom t\n",
            "Epoch 67. Time: 7.050, Train loss: 1.056\n",
            " their antipodes perhaps what wonder tha|n they person of the fire of it sometime\n",
            "Epoch 68. Time: 7.040, Train loss: 1.055\n",
            "eous bearing such a virtue may display i|n himself the rearing it has reaches of \n",
            "Epoch 69. Time: 7.101, Train loss: 1.055\n",
            "consciously hankers for his week and wor|ld why but within or are another contemp\n",
            "Epoch 70. Time: 7.050, Train loss: 1.054\n",
            "there is not genuine on the other hand t|hrough the focectism to the akame there \n",
            "Epoch 71. Time: 7.105, Train loss: 1.050\n",
            " of the course presumable nature of the |intellection of rulingly more falsedaye \n",
            "Epoch 72. Time: 7.085, Train loss: 1.050\n",
            "has dissolved the religious instincts so| we presence until a superior best svewh\n",
            "Epoch 73. Time: 7.196, Train loss: 1.050\n",
            " stunting of mankind as exemplified in t|he etope perceive perhaps strongest lain\n",
            "Epoch 74. Time: 7.020, Train loss: 1.047\n",
            "d thus where there exists no demonstrabl|e endom by nature honeent herediglists a\n",
            "Epoch 75. Time: 6.978, Train loss: 1.047\n",
            "being a general ill will to all philosop|hiable into the very stood for which tru\n",
            "Epoch 76. Time: 7.076, Train loss: 1.049\n",
            "irstly a commonplace type of man with co|nceptions for her had conscience from th\n",
            "Epoch 77. Time: 7.034, Train loss: 1.045\n",
            "s ever written the following proposition| awe feeling that that faculation in eve\n",
            "Epoch 78. Time: 7.113, Train loss: 1.044\n",
            " from the ruled or among the ruled class| of an actoration of their kind of rathe\n",
            "Epoch 79. Time: 7.121, Train loss: 1.045\n",
            "t of the life history of every philosoph|er divine for soodeng the task can be et\n",
            "Epoch 80. Time: 7.075, Train loss: 1.041\n",
            "uch courses of conduct as are the ordina|ry mabording alouse seduction would comp\n",
            "Epoch 81. Time: 7.105, Train loss: 1.041\n",
            "he always said yes with his lips even up|on which he way or it lack of schoral th\n",
            "Epoch 82. Time: 7.086, Train loss: 1.040\n",
            "hink and live otherwise namely kurmagati|on which has vullogical realtistive ther\n",
            "Epoch 83. Time: 7.072, Train loss: 1.041\n",
            "the consideration of universal utility t|o do loveral scarnapily regals are all t\n",
            "Epoch 84. Time: 7.085, Train loss: 1.041\n",
            "ment justice was not done to the importa|nts no doubt up to men philosophy one du\n",
            "Epoch 85. Time: 7.118, Train loss: 1.041\n",
            " have no doubt that as men argue in thei|r fatery away of their imperition the ge\n",
            "Epoch 86. Time: 7.139, Train loss: 1.037\n",
            " understood as one s advantage the crude| in regularity with them man be seeks we\n",
            "Epoch 87. Time: 7.026, Train loss: 1.036\n",
            "one is being cursed the familiarity of s|uithor that good countox belong to will \n",
            "Epoch 88. Time: 6.998, Train loss: 1.044\n",
            " been corrupted by marriage he who exult| which may them youth the source of whic\n",
            "Epoch 89. Time: 6.961, Train loss: 1.035\n",
            "sophers men have placed sympathy very lo|so freedom all subjection that we if tho\n",
            "Epoch 90. Time: 7.072, Train loss: 1.036\n",
            "s of all the strength which the struggle| and interrond to the commins it has pur\n",
            "Epoch 91. Time: 7.093, Train loss: 1.036\n",
            "s optimistic and insists that to knowled| preduditian profound especially endom u\n",
            "Epoch 92. Time: 7.101, Train loss: 1.035\n",
            "cholar and his social distastefulness ag|ain self stome of themselves sunking of \n",
            "Epoch 93. Time: 7.021, Train loss: 1.032\n",
            "ty need neither be amazed nor even sad a|fterwipes in art it has hitherto bad gre\n",
            "Epoch 94. Time: 7.056, Train loss: 1.030\n",
            "e absolutely contradictory just as it ar|istophited to let it he ward heart eacul\n",
            "Epoch 95. Time: 7.115, Train loss: 1.034\n",
            "in untruth the individual cannot extrica|tion in simplish and tenders in ourselve\n",
            "Epoch 96. Time: 6.984, Train loss: 1.033\n",
            "est of evils for it lengthens the ordeal| as good and men of our sense made the j\n",
            "Epoch 97. Time: 7.121, Train loss: 1.032\n",
            " by hafis and goethe the bold letting go|od religion and conneces for their final\n",
            "Epoch 98. Time: 7.139, Train loss: 1.029\n",
            "us the essence of the world would be und|erdox alone and withins further knows wh\n",
            "Epoch 99. Time: 7.088, Train loss: 1.029\n",
            "ook that said plainly enough do not diss|ibtedies have ale not dequires itself us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzUTHB9O_H6J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}